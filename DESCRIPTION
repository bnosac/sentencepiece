Package: sentencepiece
Type: Package
Title: Text Tokenization using Byte Pair Encoding and Unigram Modelling
Version: 0.1.0
Authors@R: c(
    person('Jan', 'Wijffels', role = c('aut', 'cre', 'cph'), email = 'jwijffels@bnosac.be', comment = "R wrapper"), 
    person('BNOSAC', role = 'cph', comment = "R wrapper"), 
    person('Google Inc.', role = c('ctb', 'cph'), comment = "Files at src/sentencepiece/src (Apache License, Version 2.0"),
    person('The Abseil Authors', role = c('ctb', 'cph'), comment = "Files at src/third_party/absl (Apache License, Version 2.0"),
    person('Google Inc.', role = c('ctb', 'cph'), comment = "Files at src/third_party/protobuf-lite (BSD-3 License"),
    person('Susumu Yata', role = c('ctb', 'cph'), comment = "Files at src/third_party/darts_clone (BSD-3 License"),
    person('Daisuke Okanohara', role = c('ctb', 'cph'), comment = "Files at src/third_party/esaxx (MIT License)"),
    person('Benjamin Heinzerling', role = c('ctb', 'cph'), comment = "Files data/models/nl.wiki.bpe.vs1000.d25.w2v.txt and data/models/nl.wiki.bpe.vs1000.model (MIT License)"))
Maintainer: Jan Wijffels <jwijffels@bnosac.be>
Description: Unsupervised text tokenizer allowing to perform byte pair encoding and unigram modelling. 
    Wraps the 'sentencepiece' library <https://github.com/google/sentencepiece> which provides a language independent tokenizer to split text in words and smaller subword units. 
    The techniques are explained in the paper "SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing" by Taku Kudo and John Richardson (2018) <doi:10.18653/v1/D18-2012>.
    Provides as well straightforward access to pretrained byte pair encoding models and subword embeddings trained on Wikipedia 
    as described in "BPEmb: Tokenization-free Pre-trained Subword Embeddings in 275 Languages" by Benjamin Heinzerling and Michael Strube (2018) <http://www.lrec-conf.org/proceedings/lrec2018/pdf/1049.pdf>.
URL: https://github.com/bnosac/sentencepiece
License: MPL-2.0
Encoding: UTF-8
LazyData: true
RoxygenNote: 7.1.0
Depends: R (>= 2.10)
Imports: Rcpp (>= 0.11.5)
Suggests: tokenizers.bpe
LinkingTo: Rcpp
